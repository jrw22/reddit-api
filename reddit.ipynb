{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw # Python Reddit API Wrapper\n",
    "import re\n",
    "import truststore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CLIENT_ID = 'b3CUxOmuvB9QeRasUu3Hew'\n",
    "CLIENT_SECRET = 'CqqnRJucVB500qgPSWTH9wHHx2acQQ'\n",
    "USER_AGENT = 'script:my_reddit_script:v1.0 (by /u/py_dev684)'\n",
    "\n",
    "\n",
    "truststore.inject_into_ssl()\n",
    "\n",
    "# Initialize the Reddit client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and Extract Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments_with_phrases(subreddits, patterns, comment_target=20):\n",
    "    \"\"\"\n",
    "    Fetch comments that contain a target phrase, sourced from 'new' posts from specified subreddits. Comments are sourced from new posts in each subreddit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddits: list\n",
    "        A list of subreddits to search\n",
    "    patterns: list\n",
    "        A list of search terms (regex)\n",
    "    comment_target: int\n",
    "        Stop collecting comments once X comments have been found containing a term from patterns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        A DataFrame of collected comments.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []  # List to store data before converting to DataFrame\n",
    "\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"Fetching from r/{subreddit_name}...\")\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "        post_counter = 0\n",
    "        comment_counter = 0\n",
    "        while len(data) < comment_target:\n",
    "            for submission in subreddit.new():\n",
    "                post_counter += 1\n",
    "                submission.comments.replace_more(limit=None)  # Replace \"MoreComments\" with actual comments\n",
    "                # Filter comments based on presence of target phrases\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_counter += 1\n",
    "                    matched_pattern = next((pattern for pattern in patterns if re.search(pattern, comment.body, re.IGNORECASE)), None)\n",
    "                    if matched_pattern:\n",
    "                        data.append({\n",
    "                            'Subreddit': subreddit_name,\n",
    "                            'Post Title': submission.title,\n",
    "                            'Comment Author': str(comment.author),\n",
    "                            'Comment': comment.body,\n",
    "                            'Matched Phrase': matched_pattern, \n",
    "                            'Upvotes': comment.score\n",
    "                        })\n",
    "\n",
    "                    if len(data) >= comment_target:  # If we've hit our comment target, break out\n",
    "                        break\n",
    "                print(f\"Posts checked: {post_counter}\")            \n",
    "                print(f\"Comments checked: {comment_counter}\")\n",
    "                print(f\"Relevant comments: {len(data)}\")\n",
    "                \n",
    "                if len(data) >= comment_target:  # If we've hit our comment target, stop processing\n",
    "                    break\n",
    "    print(f\"----\")\n",
    "    print(f\"Total posts checked: {post_counter}\")\n",
    "    print(f\"Total comments checked: {comment_counter}\")          \n",
    "    print(f\"Total comments collected: {len(data)}\")\n",
    "\n",
    "    # Convert the data list into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from r/finance...\n",
      "Posts checked: 1\n",
      "Comments checked: 0\n",
      "Relevant comments: 0\n",
      "Posts checked: 2\n",
      "Comments checked: 21\n",
      "Relevant comments: 0\n",
      "Posts checked: 3\n",
      "Comments checked: 36\n",
      "Relevant comments: 0\n",
      "Posts checked: 4\n",
      "Comments checked: 47\n",
      "Relevant comments: 0\n",
      "Posts checked: 5\n",
      "Comments checked: 156\n",
      "Relevant comments: 0\n",
      "Posts checked: 6\n",
      "Comments checked: 193\n",
      "Relevant comments: 2\n",
      "Posts checked: 7\n",
      "Comments checked: 196\n",
      "Relevant comments: 2\n",
      "Posts checked: 8\n",
      "Comments checked: 209\n",
      "Relevant comments: 2\n",
      "Posts checked: 9\n",
      "Comments checked: 209\n",
      "Relevant comments: 2\n",
      "Posts checked: 10\n",
      "Comments checked: 258\n",
      "Relevant comments: 2\n",
      "Posts checked: 11\n",
      "Comments checked: 337\n",
      "Relevant comments: 8\n",
      "Posts checked: 12\n",
      "Comments checked: 348\n",
      "Relevant comments: 8\n",
      "Posts checked: 13\n",
      "Comments checked: 373\n",
      "Relevant comments: 8\n",
      "Posts checked: 14\n",
      "Comments checked: 470\n",
      "Relevant comments: 8\n",
      "Posts checked: 15\n",
      "Comments checked: 517\n",
      "Relevant comments: 8\n",
      "Posts checked: 16\n",
      "Comments checked: 558\n",
      "Relevant comments: 8\n",
      "Posts checked: 17\n",
      "Comments checked: 581\n",
      "Relevant comments: 8\n",
      "Posts checked: 18\n",
      "Comments checked: 602\n",
      "Relevant comments: 8\n",
      "Posts checked: 19\n",
      "Comments checked: 682\n",
      "Relevant comments: 9\n",
      "Posts checked: 20\n",
      "Comments checked: 696\n",
      "Relevant comments: 10\n",
      "----\n",
      "Total posts checked: 20\n",
      "Total comments checked: 696\n",
      "Total comments collected: 10\n"
     ]
    }
   ],
   "source": [
    "subreddits = ['finance']\n",
    "patterns = [r'\\bHSBC\\b', r'\\bCiti\\b', r'\\bNatWest\\b', r'\\bCoutts\\b', r'\\bLloyds\\b', r'\\bBarclays\\b', r'\\bStandard\\s+Chartered\\b', r'\\bSantander\\b', r'\\bBank\\s+of\\s+England\\b', r'\\bBoE\\b', r'\\bGoldman\\s+Sachs\\b', r'\\bMorgan\\s+Stanley\\b', r'\\bSilicon\\s+Valley\\s+Bank\\b', r'\\bSVB\\b', r'\\bCredit\\s+Suisse\\b', r'\\bHalifax\\b', r'/bInvestec/b', r'\\bVirgin\\s+Money\\b']\n",
    "\n",
    "comments = fetch_comments_with_phrases(subreddits, patterns, comment_target=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Comment Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Matched Phrase</th>\n",
       "      <th>Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finance</td>\n",
       "      <td>Why Wall Street Is Suddenly Having an Everythi...</td>\n",
       "      <td>savagepanda</td>\n",
       "      <td>It’s odd how fed suddenly became dovish.  They...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finance</td>\n",
       "      <td>Why Wall Street Is Suddenly Having an Everythi...</td>\n",
       "      <td>TokyoSxWhale</td>\n",
       "      <td>The BTFP expires March 11. If they don’t raise...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>Omnipotent-Ape</td>\n",
       "      <td>In your reply, you blame regulators for stifli...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>Bocifer1</td>\n",
       "      <td>Right but just letting the industry regulate i...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>Capadvantagetutoring</td>\n",
       "      <td>Let’s clear that up. SVB Fucked up. They didn’...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>feelings_arent_facts</td>\n",
       "      <td>SVB literally collapsed because of a lack of c...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>TaxGuy_021</td>\n",
       "      <td>No?\\n\\nHistorically, bank runs happened due to...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>Kevstuf</td>\n",
       "      <td>From what I understand, SVB did not hedge thei...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>finance</td>\n",
       "      <td>I'm a reporter at Bloomberg News. We just publ...</td>\n",
       "      <td>jigsaw_faust</td>\n",
       "      <td>I work for a credit union and it’s been crazy ...</td>\n",
       "      <td>\\bSantander\\b</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>finance</td>\n",
       "      <td>Strip Clubs, Lewd Photos and a Boozy Hotel: Th...</td>\n",
       "      <td>OceanofChoco</td>\n",
       "      <td>I agree mostly, I suppose it really depends on...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subreddit                                         Post Title  \\\n",
       "0   finance  Why Wall Street Is Suddenly Having an Everythi...   \n",
       "1   finance  Why Wall Street Is Suddenly Having an Everythi...   \n",
       "2   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "3   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "4   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "5   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "6   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "7   finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "8   finance  I'm a reporter at Bloomberg News. We just publ...   \n",
       "9   finance  Strip Clubs, Lewd Photos and a Boozy Hotel: Th...   \n",
       "\n",
       "         Comment Author                                            Comment  \\\n",
       "0           savagepanda  It’s odd how fed suddenly became dovish.  They...   \n",
       "1          TokyoSxWhale  The BTFP expires March 11. If they don’t raise...   \n",
       "2        Omnipotent-Ape  In your reply, you blame regulators for stifli...   \n",
       "3              Bocifer1  Right but just letting the industry regulate i...   \n",
       "4  Capadvantagetutoring  Let’s clear that up. SVB Fucked up. They didn’...   \n",
       "5  feelings_arent_facts  SVB literally collapsed because of a lack of c...   \n",
       "6            TaxGuy_021  No?\\n\\nHistorically, bank runs happened due to...   \n",
       "7               Kevstuf  From what I understand, SVB did not hedge thei...   \n",
       "8          jigsaw_faust  I work for a credit union and it’s been crazy ...   \n",
       "9          OceanofChoco  I agree mostly, I suppose it really depends on...   \n",
       "\n",
       "        Matched Phrase  Upvotes  \n",
       "0              \\bSVB\\b       10  \n",
       "1              \\bSVB\\b        7  \n",
       "2              \\bSVB\\b       11  \n",
       "3              \\bSVB\\b        5  \n",
       "4              \\bSVB\\b        1  \n",
       "5              \\bSVB\\b        6  \n",
       "6              \\bSVB\\b        1  \n",
       "7              \\bSVB\\b        0  \n",
       "8        \\bSantander\\b       22  \n",
       "9  \\bGoldman\\s+Sachs\\b        1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts_with_phrases(subreddits, patterns, post_target=20):\n",
    "    \"\"\"\n",
    "    Fetch posts that contain a target phrase, sourced from 'new' posts from specified subreddits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddits: list\n",
    "        A list of subreddits to search\n",
    "    patterns: list\n",
    "        A list of search terms (regex)\n",
    "    posts_target: int\n",
    "        Stop collecting posts once X comments have been found containing a term from patterns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        A DataFrame of collected posts.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []  # List to store data before converting to DataFrame\n",
    "    post_counter = 0\n",
    "    \n",
    "    while len(data) < post_target:\n",
    "        \n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"Fetching from r/{subreddit_name}...\")\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            for submission in subreddit.new():\n",
    "                post_counter += 1\n",
    "                matched_pattern = next((pattern for pattern in patterns if re.search(pattern, submission.selftext, re.IGNORECASE)), None)\n",
    "                \n",
    "                if matched_pattern:\n",
    "                    data.append({\n",
    "                        'Subreddit': subreddit_name,\n",
    "                        'Post Title': submission.title,\n",
    "                        'Post Author': str(submission.author),\n",
    "                        'Post': submission.selftext,\n",
    "                        'Matched Phrase': matched_pattern, \n",
    "                        'Upvotes': submission.score\n",
    "                    })\n",
    "\n",
    "                print(f\"Posts checked: {post_counter}\")            \n",
    "                print(f\"Relevant posts: {len(data)}\")\n",
    "                \n",
    "                if len(data) >= post_target:  # If we've hit our comment target, stop processing\n",
    "                    break\n",
    "    print(f\"----\")\n",
    "    print(f\"Total posts checked: {post_counter}\")\n",
    "    print(f\"Total posts collected: {len(data)}\")\n",
    "\n",
    "    # Convert the data list into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from r/finance...\n",
      "Posts checked: 1\n",
      "Relevant posts: 0\n",
      "Posts checked: 2\n",
      "Relevant posts: 0\n",
      "Posts checked: 3\n",
      "Relevant posts: 0\n",
      "Posts checked: 4\n",
      "Relevant posts: 0\n",
      "Posts checked: 5\n",
      "Relevant posts: 0\n",
      "Posts checked: 6\n",
      "Relevant posts: 0\n",
      "Posts checked: 7\n",
      "Relevant posts: 0\n",
      "Posts checked: 8\n",
      "Relevant posts: 0\n",
      "Posts checked: 9\n",
      "Relevant posts: 0\n",
      "Posts checked: 10\n",
      "Relevant posts: 0\n",
      "Posts checked: 11\n",
      "Relevant posts: 1\n",
      "Posts checked: 12\n",
      "Relevant posts: 1\n",
      "Posts checked: 13\n",
      "Relevant posts: 1\n",
      "Posts checked: 14\n",
      "Relevant posts: 1\n",
      "Posts checked: 15\n",
      "Relevant posts: 1\n",
      "Posts checked: 16\n",
      "Relevant posts: 1\n",
      "Posts checked: 17\n",
      "Relevant posts: 1\n",
      "Posts checked: 18\n",
      "Relevant posts: 1\n",
      "Posts checked: 19\n",
      "Relevant posts: 1\n",
      "Posts checked: 20\n",
      "Relevant posts: 1\n",
      "Posts checked: 21\n",
      "Relevant posts: 1\n",
      "Posts checked: 22\n",
      "Relevant posts: 1\n",
      "Posts checked: 23\n",
      "Relevant posts: 1\n",
      "Posts checked: 24\n",
      "Relevant posts: 1\n",
      "Posts checked: 25\n",
      "Relevant posts: 1\n",
      "Posts checked: 26\n",
      "Relevant posts: 1\n",
      "Posts checked: 27\n",
      "Relevant posts: 1\n",
      "Posts checked: 28\n",
      "Relevant posts: 1\n",
      "Posts checked: 29\n",
      "Relevant posts: 1\n",
      "Posts checked: 30\n",
      "Relevant posts: 1\n",
      "Posts checked: 31\n",
      "Relevant posts: 1\n",
      "Posts checked: 32\n",
      "Relevant posts: 1\n",
      "Posts checked: 33\n",
      "Relevant posts: 1\n",
      "Posts checked: 34\n",
      "Relevant posts: 1\n",
      "Posts checked: 35\n",
      "Relevant posts: 1\n",
      "Posts checked: 36\n",
      "Relevant posts: 1\n",
      "Posts checked: 37\n",
      "Relevant posts: 1\n",
      "Posts checked: 38\n",
      "Relevant posts: 1\n",
      "Posts checked: 39\n",
      "Relevant posts: 1\n",
      "Posts checked: 40\n",
      "Relevant posts: 1\n",
      "Posts checked: 41\n",
      "Relevant posts: 1\n",
      "Posts checked: 42\n",
      "Relevant posts: 1\n",
      "Posts checked: 43\n",
      "Relevant posts: 1\n",
      "Posts checked: 44\n",
      "Relevant posts: 1\n",
      "Posts checked: 45\n",
      "Relevant posts: 1\n",
      "Posts checked: 46\n",
      "Relevant posts: 1\n",
      "Posts checked: 47\n",
      "Relevant posts: 1\n",
      "Posts checked: 48\n",
      "Relevant posts: 1\n",
      "Posts checked: 49\n",
      "Relevant posts: 1\n",
      "Posts checked: 50\n",
      "Relevant posts: 1\n",
      "Posts checked: 51\n",
      "Relevant posts: 1\n",
      "Posts checked: 52\n",
      "Relevant posts: 1\n",
      "Posts checked: 53\n",
      "Relevant posts: 1\n",
      "Posts checked: 54\n",
      "Relevant posts: 1\n",
      "Fetching from r/personalfinance...\n",
      "Posts checked: 55\n",
      "Relevant posts: 1\n",
      "Posts checked: 56\n",
      "Relevant posts: 1\n",
      "Posts checked: 57\n",
      "Relevant posts: 1\n",
      "Posts checked: 58\n",
      "Relevant posts: 1\n",
      "Posts checked: 59\n",
      "Relevant posts: 1\n",
      "Posts checked: 60\n",
      "Relevant posts: 1\n",
      "Posts checked: 61\n",
      "Relevant posts: 1\n",
      "Posts checked: 62\n",
      "Relevant posts: 1\n",
      "Posts checked: 63\n",
      "Relevant posts: 1\n",
      "Posts checked: 64\n",
      "Relevant posts: 1\n",
      "Posts checked: 65\n",
      "Relevant posts: 1\n",
      "Posts checked: 66\n",
      "Relevant posts: 2\n",
      "Posts checked: 67\n",
      "Relevant posts: 2\n",
      "Posts checked: 68\n",
      "Relevant posts: 2\n",
      "Posts checked: 69\n",
      "Relevant posts: 2\n",
      "Posts checked: 70\n",
      "Relevant posts: 2\n",
      "Posts checked: 71\n",
      "Relevant posts: 2\n",
      "Posts checked: 72\n",
      "Relevant posts: 2\n",
      "Posts checked: 73\n",
      "Relevant posts: 2\n",
      "Posts checked: 74\n",
      "Relevant posts: 2\n",
      "Posts checked: 75\n",
      "Relevant posts: 2\n",
      "Posts checked: 76\n",
      "Relevant posts: 2\n",
      "Posts checked: 77\n",
      "Relevant posts: 2\n",
      "Posts checked: 78\n",
      "Relevant posts: 2\n",
      "Posts checked: 79\n",
      "Relevant posts: 2\n",
      "Posts checked: 80\n",
      "Relevant posts: 2\n",
      "Posts checked: 81\n",
      "Relevant posts: 2\n",
      "Posts checked: 82\n",
      "Relevant posts: 2\n",
      "Posts checked: 83\n",
      "Relevant posts: 2\n",
      "Posts checked: 84\n",
      "Relevant posts: 2\n",
      "Posts checked: 85\n",
      "Relevant posts: 2\n",
      "Posts checked: 86\n",
      "Relevant posts: 2\n",
      "Posts checked: 87\n",
      "Relevant posts: 2\n",
      "Posts checked: 88\n",
      "Relevant posts: 2\n",
      "Posts checked: 89\n",
      "Relevant posts: 2\n",
      "Posts checked: 90\n",
      "Relevant posts: 2\n",
      "Posts checked: 91\n",
      "Relevant posts: 2\n",
      "Posts checked: 92\n",
      "Relevant posts: 2\n",
      "Posts checked: 93\n",
      "Relevant posts: 2\n",
      "Posts checked: 94\n",
      "Relevant posts: 2\n",
      "Posts checked: 95\n",
      "Relevant posts: 2\n",
      "Posts checked: 96\n",
      "Relevant posts: 2\n",
      "Posts checked: 97\n",
      "Relevant posts: 2\n",
      "Posts checked: 98\n",
      "Relevant posts: 2\n",
      "Posts checked: 99\n",
      "Relevant posts: 3\n",
      "Posts checked: 100\n",
      "Relevant posts: 3\n",
      "Posts checked: 101\n",
      "Relevant posts: 3\n",
      "Posts checked: 102\n",
      "Relevant posts: 3\n",
      "Posts checked: 103\n",
      "Relevant posts: 3\n",
      "Posts checked: 104\n",
      "Relevant posts: 3\n",
      "Posts checked: 105\n",
      "Relevant posts: 3\n",
      "Posts checked: 106\n",
      "Relevant posts: 3\n",
      "Posts checked: 107\n",
      "Relevant posts: 3\n",
      "Posts checked: 108\n",
      "Relevant posts: 3\n",
      "Posts checked: 109\n",
      "Relevant posts: 3\n",
      "Posts checked: 110\n",
      "Relevant posts: 3\n",
      "Posts checked: 111\n",
      "Relevant posts: 3\n",
      "Posts checked: 112\n",
      "Relevant posts: 3\n",
      "Posts checked: 113\n",
      "Relevant posts: 3\n",
      "Posts checked: 114\n",
      "Relevant posts: 3\n",
      "Posts checked: 115\n",
      "Relevant posts: 3\n",
      "Posts checked: 116\n",
      "Relevant posts: 3\n",
      "Posts checked: 117\n",
      "Relevant posts: 3\n",
      "Posts checked: 118\n",
      "Relevant posts: 3\n",
      "Posts checked: 119\n",
      "Relevant posts: 3\n",
      "Posts checked: 120\n",
      "Relevant posts: 3\n",
      "Posts checked: 121\n",
      "Relevant posts: 3\n",
      "Posts checked: 122\n",
      "Relevant posts: 3\n",
      "Posts checked: 123\n",
      "Relevant posts: 3\n",
      "Posts checked: 124\n",
      "Relevant posts: 3\n",
      "Posts checked: 125\n",
      "Relevant posts: 3\n",
      "Posts checked: 126\n",
      "Relevant posts: 3\n",
      "Posts checked: 127\n",
      "Relevant posts: 3\n",
      "Posts checked: 128\n",
      "Relevant posts: 3\n",
      "Posts checked: 129\n",
      "Relevant posts: 3\n",
      "Posts checked: 130\n",
      "Relevant posts: 3\n",
      "Posts checked: 131\n",
      "Relevant posts: 3\n",
      "Posts checked: 132\n",
      "Relevant posts: 3\n",
      "Posts checked: 133\n",
      "Relevant posts: 3\n",
      "Posts checked: 134\n",
      "Relevant posts: 3\n",
      "Posts checked: 135\n",
      "Relevant posts: 3\n",
      "Posts checked: 136\n",
      "Relevant posts: 3\n",
      "Posts checked: 137\n",
      "Relevant posts: 3\n",
      "Posts checked: 138\n",
      "Relevant posts: 3\n",
      "Posts checked: 139\n",
      "Relevant posts: 3\n",
      "Posts checked: 140\n",
      "Relevant posts: 3\n",
      "Posts checked: 141\n",
      "Relevant posts: 3\n",
      "Posts checked: 142\n",
      "Relevant posts: 3\n",
      "Posts checked: 143\n",
      "Relevant posts: 3\n",
      "Posts checked: 144\n",
      "Relevant posts: 3\n",
      "Posts checked: 145\n",
      "Relevant posts: 3\n",
      "Posts checked: 146\n",
      "Relevant posts: 3\n",
      "Posts checked: 147\n",
      "Relevant posts: 3\n",
      "Posts checked: 148\n",
      "Relevant posts: 3\n",
      "Posts checked: 149\n",
      "Relevant posts: 3\n",
      "Posts checked: 150\n",
      "Relevant posts: 3\n",
      "Posts checked: 151\n",
      "Relevant posts: 3\n",
      "Posts checked: 152\n",
      "Relevant posts: 3\n",
      "Posts checked: 153\n",
      "Relevant posts: 3\n",
      "Posts checked: 154\n",
      "Relevant posts: 3\n",
      "Fetching from r/UKPersonalFinance...\n",
      "Posts checked: 155\n",
      "Relevant posts: 3\n",
      "Posts checked: 156\n",
      "Relevant posts: 3\n",
      "Posts checked: 157\n",
      "Relevant posts: 3\n",
      "Posts checked: 158\n",
      "Relevant posts: 3\n",
      "Posts checked: 159\n",
      "Relevant posts: 3\n",
      "Posts checked: 160\n",
      "Relevant posts: 3\n",
      "Posts checked: 161\n",
      "Relevant posts: 3\n",
      "Posts checked: 162\n",
      "Relevant posts: 3\n",
      "Posts checked: 163\n",
      "Relevant posts: 3\n",
      "Posts checked: 164\n",
      "Relevant posts: 3\n",
      "Posts checked: 165\n",
      "Relevant posts: 3\n",
      "Posts checked: 166\n",
      "Relevant posts: 3\n",
      "Posts checked: 167\n",
      "Relevant posts: 3\n",
      "Posts checked: 168\n",
      "Relevant posts: 3\n",
      "Posts checked: 169\n",
      "Relevant posts: 3\n",
      "Posts checked: 170\n",
      "Relevant posts: 4\n",
      "Posts checked: 171\n",
      "Relevant posts: 4\n",
      "Posts checked: 172\n",
      "Relevant posts: 4\n",
      "Posts checked: 173\n",
      "Relevant posts: 5\n",
      "----\n",
      "Total posts checked: 173\n",
      "Total posts collected: 5\n"
     ]
    }
   ],
   "source": [
    "subreddits = ['finance', 'personalfinance', 'UKPersonalFinance']\n",
    "patterns = [r'\\bHSBC\\b', r'\\bCiti\\b', r'\\bNatWest\\b', r'\\bCoutts\\b', r'\\bLloyds\\b', r'\\bBarclays\\b', r'\\bStandard\\s+Chartered\\b', r'\\bSantander\\b', r'\\bBank\\s+of\\s+England\\b', r'\\bBoE\\b', r'\\bGoldman\\s+Sachs\\b', r'\\bMorgan\\s+Stanley\\b', r'\\bSilicon\\s+Valley\\s+Bank\\b', r'\\bSVB\\b', r'\\bCredit\\s+Suisse\\b', r'\\bHalifax\\b', r'\\bStandard\\s+Chartered\\b', r'/bInvestec/b', r'\\bVirgin\\s+Money\\b']\n",
    "\n",
    "posts = fetch_posts_with_phrases(subreddits, patterns, post_target=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Post Author</th>\n",
       "      <th>Post</th>\n",
       "      <th>Matched Phrase</th>\n",
       "      <th>Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finance</td>\n",
       "      <td>Wall Street CEOs say proposed banking rules wi...</td>\n",
       "      <td>ethereal3xp</td>\n",
       "      <td>Wall Street CEOs on Wednesday pushed back agai...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Question About Auto Refinance</td>\n",
       "      <td>iguana_lover420</td>\n",
       "      <td>Honestly I couldn't find a relevant subreddit ...</td>\n",
       "      <td>\\bSantander\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Getting Myself Out Of This Hole</td>\n",
       "      <td>traitornation</td>\n",
       "      <td>This group has helped me a lot and helped me o...</td>\n",
       "      <td>\\bBarclays\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UKPersonalFinance</td>\n",
       "      <td>Is there a better way to save my money?</td>\n",
       "      <td>KiloRGB</td>\n",
       "      <td>Hi all,\\n\\nCurrently saving up to pay my car o...</td>\n",
       "      <td>\\bLloyds\\b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UKPersonalFinance</td>\n",
       "      <td>Made a mistake with a balance transfer credit ...</td>\n",
       "      <td>EntranceTiny6943</td>\n",
       "      <td>Hi, \\n\\nI have recently taken on a new credit ...</td>\n",
       "      <td>\\bHSBC\\b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Subreddit                                         Post Title  \\\n",
       "0            finance  Wall Street CEOs say proposed banking rules wi...   \n",
       "1    personalfinance                      Question About Auto Refinance   \n",
       "2    personalfinance                    Getting Myself Out Of This Hole   \n",
       "3  UKPersonalFinance            Is there a better way to save my money?   \n",
       "4  UKPersonalFinance  Made a mistake with a balance transfer credit ...   \n",
       "\n",
       "        Post Author                                               Post  \\\n",
       "0       ethereal3xp  Wall Street CEOs on Wednesday pushed back agai...   \n",
       "1   iguana_lover420  Honestly I couldn't find a relevant subreddit ...   \n",
       "2     traitornation  This group has helped me a lot and helped me o...   \n",
       "3           KiloRGB  Hi all,\\n\\nCurrently saving up to pay my car o...   \n",
       "4  EntranceTiny6943  Hi, \\n\\nI have recently taken on a new credit ...   \n",
       "\n",
       "        Matched Phrase  Upvotes  \n",
       "0  \\bGoldman\\s+Sachs\\b      285  \n",
       "1        \\bSantander\\b        1  \n",
       "2         \\bBarclays\\b        1  \n",
       "3           \\bLloyds\\b        1  \n",
       "4             \\bHSBC\\b        0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jackwalker/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load sentiment analyser\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialise VADER\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Comment Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Matched Phrase</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finance</td>\n",
       "      <td>‘Almost All Loans Are Bad’—Why Banks Aren’t Le...</td>\n",
       "      <td>hcbaron</td>\n",
       "      <td>Extracted article:\\n\\nBanks would love to lend...</td>\n",
       "      <td>\\bBarclays\\b</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finance</td>\n",
       "      <td>Moronic Monday - September 05, 2023 - Your Wee...</td>\n",
       "      <td>14446368</td>\n",
       "      <td>Could we un-pin the SVB thing now?</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finance</td>\n",
       "      <td>Bloomberg overhauls management team with Mark ...</td>\n",
       "      <td>marketrent</td>\n",
       "      <td>Per an internal memo sent by founder Mike Bloo...</td>\n",
       "      <td>\\bBank\\s+of\\s+England\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finance</td>\n",
       "      <td>[Bloomberg] New York and California Each Lost ...</td>\n",
       "      <td>mzachi</td>\n",
       "      <td>here's some quotes from the article for those ...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>34</td>\n",
       "      <td>{'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>finance</td>\n",
       "      <td>Is David Solomon Too Big a Jerk to Run Goldman...</td>\n",
       "      <td>MartianActual</td>\n",
       "      <td>Counterpoint: What other kind of person would ...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>18</td>\n",
       "      <td>{'neg': 0.179, 'neu': 0.821, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finance</td>\n",
       "      <td>Moody's downgrades US banks, warns of possible...</td>\n",
       "      <td>bellayang1216</td>\n",
       "      <td>Based on the information provided:\\r  \\n\\r  \\n...</td>\n",
       "      <td>\\bSilicon\\s+Valley\\s+Bank\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.129, 'neu': 0.781, 'pos': 0.09, 'com...</td>\n",
       "      <td>-0.9558</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finance</td>\n",
       "      <td>Why the US is interested in audits of Chinese ...</td>\n",
       "      <td>asuka_rice</td>\n",
       "      <td>Plenty of lemons everywhere.\\n\\nCS, SVB and FT...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.129, 'neu': 0.72, 'pos': 0.151, 'com...</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>finance</td>\n",
       "      <td>‘Success fees’ and thirsty emails: inside a $9...</td>\n",
       "      <td>FishFar4370</td>\n",
       "      <td>Musk's desire to have an emotional tantrum and...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.038, 'neu': 0.829, 'pos': 0.134, 'co...</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>finance</td>\n",
       "      <td>‘Almost All Loans Are Bad’—Why Banks Aren’t Le...</td>\n",
       "      <td>hcbaron</td>\n",
       "      <td>Extracted article:\\n\\nBanks would love to lend...</td>\n",
       "      <td>\\bBarclays\\b</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>finance</td>\n",
       "      <td>Moronic Monday - September 05, 2023 - Your Wee...</td>\n",
       "      <td>14446368</td>\n",
       "      <td>Could we un-pin the SVB thing now?</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>finance</td>\n",
       "      <td>Bloomberg overhauls management team with Mark ...</td>\n",
       "      <td>marketrent</td>\n",
       "      <td>Per an internal memo sent by founder Mike Bloo...</td>\n",
       "      <td>\\bBank\\s+of\\s+England\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>finance</td>\n",
       "      <td>[Bloomberg] New York and California Each Lost ...</td>\n",
       "      <td>mzachi</td>\n",
       "      <td>here's some quotes from the article for those ...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>32</td>\n",
       "      <td>{'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>finance</td>\n",
       "      <td>Is David Solomon Too Big a Jerk to Run Goldman...</td>\n",
       "      <td>MartianActual</td>\n",
       "      <td>Counterpoint: What other kind of person would ...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>20</td>\n",
       "      <td>{'neg': 0.179, 'neu': 0.821, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>finance</td>\n",
       "      <td>Moody's downgrades US banks, warns of possible...</td>\n",
       "      <td>bellayang1216</td>\n",
       "      <td>Based on the information provided:\\r  \\n\\r  \\n...</td>\n",
       "      <td>\\bSilicon\\s+Valley\\s+Bank\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.129, 'neu': 0.781, 'pos': 0.09, 'com...</td>\n",
       "      <td>-0.9558</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>finance</td>\n",
       "      <td>Why the US is interested in audits of Chinese ...</td>\n",
       "      <td>asuka_rice</td>\n",
       "      <td>Plenty of lemons everywhere.\\n\\nCS, SVB and FT...</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.129, 'neu': 0.72, 'pos': 0.151, 'com...</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>finance</td>\n",
       "      <td>‘Success fees’ and thirsty emails: inside a $9...</td>\n",
       "      <td>FishFar4370</td>\n",
       "      <td>Musk's desire to have an emotional tantrum and...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>5</td>\n",
       "      <td>{'neg': 0.038, 'neu': 0.829, 'pos': 0.134, 'co...</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>finance</td>\n",
       "      <td>‘Almost All Loans Are Bad’—Why Banks Aren’t Le...</td>\n",
       "      <td>hcbaron</td>\n",
       "      <td>Extracted article:\\n\\nBanks would love to lend...</td>\n",
       "      <td>\\bBarclays\\b</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>finance</td>\n",
       "      <td>Moronic Monday - September 05, 2023 - Your Wee...</td>\n",
       "      <td>14446368</td>\n",
       "      <td>Could we un-pin the SVB thing now?</td>\n",
       "      <td>\\bSVB\\b</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>finance</td>\n",
       "      <td>Bloomberg overhauls management team with Mark ...</td>\n",
       "      <td>marketrent</td>\n",
       "      <td>Per an internal memo sent by founder Mike Bloo...</td>\n",
       "      <td>\\bBank\\s+of\\s+England\\b</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>finance</td>\n",
       "      <td>[Bloomberg] New York and California Each Lost ...</td>\n",
       "      <td>mzachi</td>\n",
       "      <td>here's some quotes from the article for those ...</td>\n",
       "      <td>\\bGoldman\\s+Sachs\\b</td>\n",
       "      <td>32</td>\n",
       "      <td>{'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subreddit                                         Post Title  \\\n",
       "0    finance  ‘Almost All Loans Are Bad’—Why Banks Aren’t Le...   \n",
       "1    finance  Moronic Monday - September 05, 2023 - Your Wee...   \n",
       "2    finance  Bloomberg overhauls management team with Mark ...   \n",
       "3    finance  [Bloomberg] New York and California Each Lost ...   \n",
       "4    finance  Is David Solomon Too Big a Jerk to Run Goldman...   \n",
       "5    finance  Moody's downgrades US banks, warns of possible...   \n",
       "6    finance  Why the US is interested in audits of Chinese ...   \n",
       "7    finance  ‘Success fees’ and thirsty emails: inside a $9...   \n",
       "8    finance  ‘Almost All Loans Are Bad’—Why Banks Aren’t Le...   \n",
       "9    finance  Moronic Monday - September 05, 2023 - Your Wee...   \n",
       "10   finance  Bloomberg overhauls management team with Mark ...   \n",
       "11   finance  [Bloomberg] New York and California Each Lost ...   \n",
       "12   finance  Is David Solomon Too Big a Jerk to Run Goldman...   \n",
       "13   finance  Moody's downgrades US banks, warns of possible...   \n",
       "14   finance  Why the US is interested in audits of Chinese ...   \n",
       "15   finance  ‘Success fees’ and thirsty emails: inside a $9...   \n",
       "16   finance  ‘Almost All Loans Are Bad’—Why Banks Aren’t Le...   \n",
       "17   finance  Moronic Monday - September 05, 2023 - Your Wee...   \n",
       "18   finance  Bloomberg overhauls management team with Mark ...   \n",
       "19   finance  [Bloomberg] New York and California Each Lost ...   \n",
       "\n",
       "   Comment Author                                            Comment  \\\n",
       "0         hcbaron  Extracted article:\\n\\nBanks would love to lend...   \n",
       "1        14446368                 Could we un-pin the SVB thing now?   \n",
       "2      marketrent  Per an internal memo sent by founder Mike Bloo...   \n",
       "3          mzachi  here's some quotes from the article for those ...   \n",
       "4   MartianActual  Counterpoint: What other kind of person would ...   \n",
       "5   bellayang1216  Based on the information provided:\\r  \\n\\r  \\n...   \n",
       "6      asuka_rice  Plenty of lemons everywhere.\\n\\nCS, SVB and FT...   \n",
       "7     FishFar4370  Musk's desire to have an emotional tantrum and...   \n",
       "8         hcbaron  Extracted article:\\n\\nBanks would love to lend...   \n",
       "9        14446368                 Could we un-pin the SVB thing now?   \n",
       "10     marketrent  Per an internal memo sent by founder Mike Bloo...   \n",
       "11         mzachi  here's some quotes from the article for those ...   \n",
       "12  MartianActual  Counterpoint: What other kind of person would ...   \n",
       "13  bellayang1216  Based on the information provided:\\r  \\n\\r  \\n...   \n",
       "14     asuka_rice  Plenty of lemons everywhere.\\n\\nCS, SVB and FT...   \n",
       "15    FishFar4370  Musk's desire to have an emotional tantrum and...   \n",
       "16        hcbaron  Extracted article:\\n\\nBanks would love to lend...   \n",
       "17       14446368                 Could we un-pin the SVB thing now?   \n",
       "18     marketrent  Per an internal memo sent by founder Mike Bloo...   \n",
       "19         mzachi  here's some quotes from the article for those ...   \n",
       "\n",
       "                 Matched Phrase  Upvotes  \\\n",
       "0                  \\bBarclays\\b       13   \n",
       "1                       \\bSVB\\b        1   \n",
       "2       \\bBank\\s+of\\s+England\\b        2   \n",
       "3           \\bGoldman\\s+Sachs\\b       34   \n",
       "4           \\bGoldman\\s+Sachs\\b       18   \n",
       "5   \\bSilicon\\s+Valley\\s+Bank\\b        2   \n",
       "6                       \\bSVB\\b        2   \n",
       "7           \\bGoldman\\s+Sachs\\b        6   \n",
       "8                  \\bBarclays\\b       13   \n",
       "9                       \\bSVB\\b        1   \n",
       "10      \\bBank\\s+of\\s+England\\b        2   \n",
       "11          \\bGoldman\\s+Sachs\\b       32   \n",
       "12          \\bGoldman\\s+Sachs\\b       20   \n",
       "13  \\bSilicon\\s+Valley\\s+Bank\\b        2   \n",
       "14                      \\bSVB\\b        2   \n",
       "15          \\bGoldman\\s+Sachs\\b        5   \n",
       "16                 \\bBarclays\\b       13   \n",
       "17                      \\bSVB\\b        1   \n",
       "18      \\bBank\\s+of\\s+England\\b        2   \n",
       "19          \\bGoldman\\s+Sachs\\b       32   \n",
       "\n",
       "                                     sentiment_scores  compound sentiment  \n",
       "0   {'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...    0.9977  POSITIVE  \n",
       "1   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   NEUTRAL  \n",
       "2   {'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...    0.1860  POSITIVE  \n",
       "3   {'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...    0.6953  POSITIVE  \n",
       "4   {'neg': 0.179, 'neu': 0.821, 'pos': 0.0, 'comp...   -0.3400  NEGATIVE  \n",
       "5   {'neg': 0.129, 'neu': 0.781, 'pos': 0.09, 'com...   -0.9558  NEGATIVE  \n",
       "6   {'neg': 0.129, 'neu': 0.72, 'pos': 0.151, 'com...    0.2249  POSITIVE  \n",
       "7   {'neg': 0.038, 'neu': 0.829, 'pos': 0.134, 'co...    0.8981  POSITIVE  \n",
       "8   {'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...    0.9977  POSITIVE  \n",
       "9   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   NEUTRAL  \n",
       "10  {'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...    0.1860  POSITIVE  \n",
       "11  {'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...    0.6953  POSITIVE  \n",
       "12  {'neg': 0.179, 'neu': 0.821, 'pos': 0.0, 'comp...   -0.3400  NEGATIVE  \n",
       "13  {'neg': 0.129, 'neu': 0.781, 'pos': 0.09, 'com...   -0.9558  NEGATIVE  \n",
       "14  {'neg': 0.129, 'neu': 0.72, 'pos': 0.151, 'com...    0.2249  POSITIVE  \n",
       "15  {'neg': 0.038, 'neu': 0.829, 'pos': 0.134, 'co...    0.8981  POSITIVE  \n",
       "16  {'neg': 0.049, 'neu': 0.799, 'pos': 0.152, 'co...    0.9977  POSITIVE  \n",
       "17  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   NEUTRAL  \n",
       "18  {'neg': 0.021, 'neu': 0.941, 'pos': 0.037, 'co...    0.1860  POSITIVE  \n",
       "19  {'neg': 0.032, 'neu': 0.916, 'pos': 0.051, 'co...    0.6953  POSITIVE  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply VADER analysis on text column\n",
    "df['sentiment_scores'] = df['Comment'].apply(lambda x: sia.polarity_scores(x))\n",
    "df['compound'] = df['sentiment_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "df['sentiment'] = df['compound'].apply(lambda c: 'POSITIVE' if c >= 0.05 else ('NEGATIVE' if c <= -0.05 else 'NEUTRAL'))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reddit_comments_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)kust/finbert-tone/resolve/main/vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 8.84MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
